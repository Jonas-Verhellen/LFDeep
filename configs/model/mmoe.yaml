
_target_: project.model.mixture_of_experts.MMoE

config:
  num_tasks: 641
  num_experts: 2
  num_layers: 1
  num_units: 5

  sequence_len: 100
  num_features: 1278

  use_expert_bias: no
  use_gate_bias: no 

  expert: configs/model/tcn.yaml
  optimizer: configs/optim/adam.yaml
