
_target_: project.model.mixture_of_experts.MMoE

config:
  num_tasks: 641
  num_experts: 5
  num_units: 10

  sequence_len: 100
  num_features: 1918
  #lstm_layers: 2
  #lstm_hidden: 650

  use_expert_bias: yes
  use_gate_bias: yes

  expert: configs/model/tcn.yaml
  optimizer: configs/optim/adam.yaml
